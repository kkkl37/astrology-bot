{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/astrology-bot/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig, TrainingArguments, pipeline, logging, LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 0.000000GB\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "def display_cuda_memory():    \n",
    "    print(\"\\n--------------------------------------------------\\n\")\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "display_cuda_memory()\n",
    "\n",
    "#For PyTorch memory management add the following code\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, dataset, and new model name\n",
    "base_model = 'meta-llama/Llama-2-7b-hf'\n",
    "horoscope_dataset = \"chloeliu/horoscope\"\n",
    "new_model = \"llama-2-7b-chat-horoscope\"\n",
    "\n",
    "# credentials\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(horoscope_dataset, split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model,token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is the love horoscope for Aquarius today.\n",
      "---\n",
      "Response:\n",
      "You can have it all, but you can't have it all at once.\n",
      "---\n",
      "What is the love horoscope for Pisces today.\n",
      "---\n",
      "Response:\n",
      "You can have it all, but you can't have it all at once.\n",
      "---\n",
      "What is the love horoscope for Aries today.\n",
      "---\n",
      "Response:\n",
      "You can have it all, but you can't have it all at once.\n",
      "---\n",
      "What is the love horoscope for Taurus today.\n",
      "---\n",
      "Response:\n",
      "You can have it all, but you can't have it all at once.\n",
      "---\n",
      "What is the love horoscope for Gemini today.\n",
      "---\n",
      "Response:\n",
      "You can have it all, but you can't have it all at once.\n",
      "---\n",
      "What is the love horoscope for Cancer today.\n",
      "---\n",
      "Response:\n",
      "You can have it all, but you can't have it all at once.\n",
      "---\n",
      "What is the love horoscope for Leo today.\n",
      "---\n",
      "Response:\n",
      "You can have it all, but you can't have it all at once.\n",
      "---\n",
      "What is the love horoscope\n"
     ]
    }
   ],
   "source": [
    "# model = LlamaForCausalLM.from_pretrained(base_model,torch_dtype=torch.float16, token=hf_token).to(device) \n",
    "\n",
    "# eval_prompt = \"\"\"\n",
    "# What is the love horoscope for Aquarius today.\n",
    "# ---\n",
    "# Response:\n",
    "# \"\"\"\n",
    "\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# 4-bit Quantization Configuration\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                  bnb_4bit_quant_type=\"nf4\", \n",
    "                                  bnb_4bit_compute_dtype=compute_dtype, \n",
    "                                  bnb_4bit_use_double_quant=False)\n",
    "\n",
    "# Load model with 4-bit precision\n",
    "model = LlamaForCausalLM.from_pretrained(base_model, quantization_config=quant_config, device_map={\"\": 0},token = hf_token)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, token = hf_token)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model, trust_remote_code=True, token = hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/astrology-bot/env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/workspaces/astrology-bot/env/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='730' max='730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [730/730 16:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.562900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.733500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.597500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.103100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.883200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.772900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.580600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.571400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=730, training_loss=1.3341605911516163, metrics={'train_runtime': 990.647, 'train_samples_per_second': 2.917, 'train_steps_per_second': 0.737, 'total_flos': 1.3478140887687168e+16, 'train_loss': 1.3341605911516163, 'epoch': 10.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set PEFT Parameters\n",
    "peft_params = LoraConfig(lora_alpha=16, lora_dropout=0.1, r=64, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "\n",
    "# Define training parameters\n",
    "training_params = TrainingArguments(output_dir=\"./results\", \n",
    "                                    num_train_epochs=10, \n",
    "                                    per_device_train_batch_size=4, \n",
    "                                    gradient_accumulation_steps=1, \n",
    "                                    optim=\"paged_adamw_32bit\", \n",
    "                                    save_steps=25, \n",
    "                                    logging_steps=25, \n",
    "                                    learning_rate=2e-4, \n",
    "                                    weight_decay=0.001, \n",
    "                                    fp16=False, \n",
    "                                    bf16=False, \n",
    "                                    max_grad_norm=0.3, \n",
    "                                    max_steps=-1, \n",
    "                                    warmup_ratio=0.03, \n",
    "                                    group_by_length=True, \n",
    "                                    lr_scheduler_type=\"constant\", \n",
    "                                    report_to=\"tensorboard\")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = SFTTrainer(model=model, \n",
    "                     train_dataset=dataset, \n",
    "                     peft_config=peft_params, \n",
    "                     dataset_text_field=\"text\", \n",
    "                     max_seq_length=None, \n",
    "                     tokenizer=tokenizer, \n",
    "                     args=training_params, \n",
    "                     packing=False)\n",
    "\n",
    "#Force clean the pytorch cache\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is the work horoscope for Aquarius today? [/INST] The workhorse in you wants to charge forward and make a name for yourself. Take the lead as the moon in Leo opposes Pluto in Aquarius. This cosmic climate will empower you to be the star of the show. Your limelight could come in the form of a promotion, a raise, or just being seen as the go-getter that you are. [/INST] $3.00\n",
      "[LEO] Workhorse shenanigans...\n",
      "[LEO] Workhorse shenanigans... Don’t be afraid to be a little silly at work today. It’s all good. The moon in Leo opposes Pluto in Aquarius, which is a perfect cosmic climate for you to be seen as a star. [/LEO] $3.00\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)\n",
    "\n",
    "# Test the model\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "prompt = \"What is the work horoscope for Aquarius today?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    token = hf_token\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00003.safetensors:   0%|          | 442k/4.95G [00:00<19:08, 4.31MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   0%|          | 9.57M/4.95G [00:00<01:30, 54.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:   0%|          | 16.0M/4.95G [00:00<02:52, 28.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   1%|          | 32.0M/4.95G [00:00<01:57, 41.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   1%|          | 48.0M/4.95G [00:01<01:45, 46.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   1%|▏         | 64.0M/4.95G [00:01<01:39, 49.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:   2%|▏         | 80.0M/4.95G [00:01<01:49, 44.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   2%|▏         | 96.0M/4.95G [00:02<01:44, 46.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   3%|▎         | 128M/4.95G [00:02<01:23, 57.9MB/s] \n",
      "\n",
      "model-00002-of-00003.safetensors:   3%|▎         | 144M/4.95G [00:02<01:20, 59.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:   3%|▎         | 160M/4.95G [00:03<01:23, 57.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:   4%|▎         | 176M/4.95G [00:03<01:27, 54.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   4%|▍         | 192M/4.95G [00:03<01:25, 55.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   4%|▍         | 208M/4.95G [00:04<01:39, 47.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   4%|▍         | 222M/4.95G [00:04<01:21, 57.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:   5%|▍         | 229M/4.95G [00:04<01:31, 51.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   5%|▍         | 240M/4.95G [00:04<01:40, 47.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   5%|▌         | 256M/4.95G [00:05<01:39, 47.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   5%|▌         | 272M/4.95G [00:05<01:32, 50.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   6%|▌         | 288M/4.95G [00:05<01:28, 52.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   6%|▌         | 296M/4.95G [00:05<01:22, 56.2MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   6%|▌         | 304M/4.95G [00:06<01:35, 48.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   6%|▋         | 320M/4.95G [00:06<01:26, 53.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   7%|▋         | 352M/4.95G [00:06<01:25, 54.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   7%|▋         | 363M/4.95G [00:06<01:13, 62.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   7%|▋         | 371M/4.95G [00:07<01:18, 58.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   8%|▊         | 384M/4.95G [00:07<01:32, 49.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   8%|▊         | 391M/4.95G [00:07<01:26, 52.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:   8%|▊         | 400M/4.95G [00:07<01:47, 42.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   8%|▊         | 416M/4.95G [00:08<01:33, 48.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   9%|▊         | 425M/4.95G [00:08<01:22, 54.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   9%|▉         | 447M/4.95G [00:08<01:10, 63.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:   9%|▉         | 455M/4.95G [00:09<01:48, 41.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:   9%|▉         | 464M/4.95G [00:09<01:57, 38.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  10%|▉         | 480M/4.95G [00:09<01:42, 43.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  10%|▉         | 490M/4.95G [00:09<01:26, 51.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  10%|█         | 497M/4.95G [00:10<01:56, 38.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  10%|█         | 512M/4.95G [00:10<01:51, 39.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  11%|█         | 528M/4.95G [00:10<01:41, 43.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  11%|█         | 544M/4.95G [00:11<01:32, 47.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  11%|█▏        | 559M/4.95G [00:11<01:12, 60.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  12%|█▏        | 576M/4.95G [00:11<01:26, 50.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  12%|█▏        | 590M/4.95G [00:11<01:07, 64.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  12%|█▏        | 598M/4.95G [00:11<01:10, 61.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  12%|█▏        | 608M/4.95G [00:12<01:24, 51.2MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  13%|█▎        | 624M/4.95G [00:12<01:17, 55.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  13%|█▎        | 640M/4.95G [00:12<01:18, 55.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  13%|█▎        | 656M/4.95G [00:12<01:09, 61.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  14%|█▍        | 688M/4.95G [00:13<01:10, 60.2MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  14%|█▍        | 704M/4.95G [00:13<01:06, 63.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  15%|█▍        | 720M/4.95G [00:13<01:07, 62.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  15%|█▍        | 736M/4.95G [00:14<01:05, 64.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  15%|█▌        | 752M/4.95G [00:14<01:01, 68.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  16%|█▌        | 768M/4.95G [00:14<01:05, 64.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  16%|█▌        | 784M/4.95G [00:14<01:07, 61.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  16%|█▌        | 800M/4.95G [00:15<01:16, 54.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  16%|█▋        | 816M/4.95G [00:15<01:11, 57.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  17%|█▋        | 832M/4.95G [00:15<01:10, 58.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  17%|█▋        | 842M/4.95G [00:15<01:04, 64.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  17%|█▋        | 850M/4.95G [00:16<01:13, 55.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  17%|█▋        | 864M/4.95G [00:16<01:17, 52.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  18%|█▊        | 880M/4.95G [00:16<01:18, 52.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  18%|█▊        | 912M/4.95G [00:17<01:17, 51.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  19%|█▉        | 928M/4.95G [00:17<01:21, 49.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  19%|█▉        | 944M/4.95G [00:17<01:16, 52.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  19%|█▉        | 960M/4.95G [00:18<01:13, 54.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  20%|█▉        | 976M/4.95G [00:18<01:08, 58.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  20%|██        | 992M/4.95G [00:18<01:15, 52.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  21%|██        | 1.04G/4.95G [00:19<01:12, 53.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  21%|██▏       | 1.06G/4.95G [00:20<01:16, 50.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  22%|██▏       | 1.07G/4.95G [00:20<01:14, 51.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  22%|██▏       | 1.09G/4.95G [00:20<01:07, 57.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  23%|██▎       | 1.12G/4.95G [00:20<00:56, 68.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  23%|██▎       | 1.14G/4.95G [00:21<01:24, 45.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  23%|██▎       | 1.15G/4.95G [00:23<04:19, 14.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  23%|██▎       | 1.16G/4.95G [00:24<03:28, 18.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  24%|██▎       | 1.17G/4.95G [00:24<03:11, 19.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  24%|██▍       | 1.18G/4.95G [00:25<04:01, 15.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  24%|██▍       | 1.20G/4.95G [00:25<02:53, 21.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  25%|██▍       | 1.22G/4.95G [00:26<02:16, 27.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  25%|██▍       | 1.23G/4.95G [00:26<01:49, 34.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  25%|██▌       | 1.25G/4.95G [00:26<01:36, 38.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  26%|██▌       | 1.26G/4.95G [00:26<01:25, 42.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  26%|██▌       | 1.28G/4.95G [00:27<01:16, 47.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  26%|██▌       | 1.30G/4.95G [00:27<01:14, 48.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  27%|██▋       | 1.33G/4.95G [00:28<01:06, 54.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  27%|██▋       | 1.33G/4.95G [00:28<01:04, 56.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  27%|██▋       | 1.34G/4.95G [00:28<01:16, 47.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  28%|██▊       | 1.37G/4.95G [00:28<01:08, 52.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  28%|██▊       | 1.38G/4.95G [00:28<01:00, 59.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  28%|██▊       | 1.38G/4.95G [00:29<01:18, 45.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  28%|██▊       | 1.39G/4.95G [00:29<01:27, 40.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  28%|██▊       | 1.41G/4.95G [00:29<01:02, 57.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  29%|██▊       | 1.41G/4.95G [00:29<01:18, 45.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  29%|██▉       | 1.42G/4.95G [00:30<01:20, 43.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  29%|██▉       | 1.44G/4.95G [00:30<01:10, 49.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  29%|██▉       | 1.46G/4.95G [00:30<01:20, 43.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  30%|██▉       | 1.47G/4.95G [00:30<01:09, 49.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  30%|███       | 1.49G/4.95G [00:31<01:03, 54.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  30%|███       | 1.50G/4.95G [00:31<01:05, 52.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  31%|███       | 1.52G/4.95G [00:31<01:06, 51.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  31%|███       | 1.54G/4.95G [00:32<01:02, 54.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  31%|███▏      | 1.55G/4.95G [00:32<01:01, 55.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  32%|███▏      | 1.57G/4.95G [00:32<01:00, 56.2MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  32%|███▏      | 1.58G/4.95G [00:32<01:01, 54.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  32%|███▏      | 1.60G/4.95G [00:33<01:00, 55.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  33%|███▎      | 1.62G/4.95G [00:33<00:57, 57.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  33%|███▎      | 1.65G/4.95G [00:34<01:00, 54.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  34%|███▎      | 1.66G/4.95G [00:34<01:02, 52.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  34%|███▍      | 1.68G/4.95G [00:34<00:58, 55.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  34%|███▍      | 1.70G/4.95G [00:34<00:56, 57.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  35%|███▍      | 1.71G/4.95G [00:35<00:53, 60.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  35%|███▍      | 1.73G/4.95G [00:35<00:48, 66.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  35%|███▌      | 1.74G/4.95G [00:36<01:52, 28.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  36%|███▌      | 1.76G/4.95G [00:36<01:26, 36.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  36%|███▌      | 1.77G/4.95G [00:36<01:26, 36.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  36%|███▌      | 1.78G/4.95G [00:37<01:26, 36.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  36%|███▌      | 1.78G/4.95G [00:37<01:21, 38.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  36%|███▌      | 1.79G/4.95G [00:37<01:22, 38.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  37%|███▋      | 1.82G/4.95G [00:38<01:01, 50.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  38%|███▊      | 1.86G/4.95G [00:38<00:53, 58.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  38%|███▊      | 1.87G/4.95G [00:38<00:56, 54.2MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  38%|███▊      | 1.89G/4.95G [00:39<00:49, 61.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  38%|███▊      | 1.90G/4.95G [00:39<00:53, 57.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  39%|███▉      | 1.92G/4.95G [00:39<00:51, 58.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  39%|███▉      | 1.94G/4.95G [00:40<00:59, 50.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  39%|███▉      | 1.95G/4.95G [00:40<01:01, 48.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  40%|████      | 1.98G/4.95G [00:41<01:06, 44.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  40%|████      | 2.00G/4.95G [00:41<01:03, 46.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  41%|████      | 2.02G/4.95G [00:41<00:56, 51.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  41%|████      | 2.03G/4.95G [00:42<01:01, 47.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  42%|████▏     | 2.06G/4.95G [00:42<00:53, 54.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  42%|████▏     | 2.08G/4.95G [00:42<00:52, 54.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  42%|████▏     | 2.10G/4.95G [00:43<00:53, 53.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  43%|████▎     | 2.11G/4.95G [00:43<00:49, 56.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  43%|████▎     | 2.13G/4.95G [00:43<00:57, 49.2MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  43%|████▎     | 2.14G/4.95G [00:44<00:59, 47.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  44%|████▎     | 2.16G/4.95G [00:44<00:46, 59.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  44%|████▍     | 2.17G/4.95G [00:44<00:48, 56.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  44%|████▍     | 2.18G/4.95G [00:44<00:57, 48.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  44%|████▍     | 2.19G/4.95G [00:45<00:48, 56.2MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  45%|████▌     | 2.24G/4.95G [00:45<00:45, 59.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  46%|████▌     | 2.26G/4.95G [00:46<00:42, 63.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  50%|████▉     | 2.46G/4.95G [00:49<00:45, 54.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  50%|█████     | 2.48G/4.95G [00:50<01:26, 28.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  50%|█████     | 2.50G/4.95G [00:51<01:15, 32.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  51%|█████     | 2.51G/4.95G [00:51<01:10, 34.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  51%|█████     | 2.53G/4.95G [00:51<01:03, 38.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  51%|█████▏    | 2.54G/4.95G [00:52<01:03, 37.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  52%|█████▏    | 2.56G/4.95G [00:52<01:05, 36.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  52%|█████▏    | 2.58G/4.95G [00:53<00:58, 40.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  53%|█████▎    | 2.61G/4.95G [00:53<00:44, 53.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  53%|█████▎    | 2.62G/4.95G [00:53<00:43, 53.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  53%|█████▎    | 2.64G/4.95G [00:54<00:43, 52.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  54%|█████▎    | 2.65G/4.95G [00:54<00:36, 62.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  54%|█████▍    | 2.67G/4.95G [00:54<00:46, 48.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  54%|█████▍    | 2.69G/4.95G [00:55<00:45, 49.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  55%|█████▍    | 2.72G/4.95G [00:55<00:37, 58.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  55%|█████▌    | 2.74G/4.95G [00:56<00:52, 41.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  56%|█████▌    | 2.75G/4.95G [00:56<00:49, 44.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  56%|█████▌    | 2.77G/4.95G [00:56<00:47, 45.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  56%|█████▋    | 2.78G/4.95G [00:57<00:43, 50.2MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  57%|█████▋    | 2.80G/4.95G [00:57<00:44, 48.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  57%|█████▋    | 2.82G/4.95G [00:57<00:38, 54.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  57%|█████▋    | 2.83G/4.95G [00:57<00:38, 55.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  58%|█████▊    | 2.85G/4.95G [00:58<00:38, 55.2MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  58%|█████▊    | 2.86G/4.95G [00:58<00:36, 56.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  58%|█████▊    | 2.88G/4.95G [00:58<00:35, 57.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  59%|█████▊    | 2.90G/4.95G [00:58<00:34, 60.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  59%|█████▉    | 2.93G/4.95G [00:59<00:34, 58.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  59%|█████▉    | 2.94G/4.95G [00:59<00:33, 60.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  60%|█████▉    | 2.94G/4.95G [00:59<00:39, 51.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  60%|█████▉    | 2.96G/4.95G [01:00<00:35, 56.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  60%|██████    | 2.98G/4.95G [01:00<00:34, 57.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  60%|██████    | 2.99G/4.95G [01:00<00:32, 59.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  61%|██████    | 3.01G/4.95G [01:00<00:31, 62.2MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  61%|██████▏   | 3.04G/4.95G [01:01<00:28, 66.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  62%|██████▏   | 3.07G/4.95G [01:01<00:28, 66.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  62%|██████▏   | 3.09G/4.95G [01:02<00:32, 56.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  63%|██████▎   | 3.10G/4.95G [01:02<00:32, 57.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  63%|██████▎   | 3.12G/4.95G [01:02<00:39, 45.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  63%|██████▎   | 3.13G/4.95G [01:03<00:32, 56.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  64%|██████▎   | 3.14G/4.95G [01:03<00:35, 50.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  64%|██████▎   | 3.15G/4.95G [01:03<00:33, 53.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  64%|██████▍   | 3.16G/4.95G [01:03<00:32, 54.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  64%|██████▍   | 3.17G/4.95G [01:03<00:29, 60.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  64%|██████▍   | 3.18G/4.95G [01:03<00:28, 62.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  65%|██████▍   | 3.20G/4.95G [01:04<00:30, 56.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  65%|██████▌   | 3.22G/4.95G [01:04<00:29, 58.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  65%|██████▌   | 3.23G/4.95G [01:04<00:29, 57.4MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  66%|██████▌   | 3.25G/4.95G [01:05<00:28, 59.8MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  66%|██████▌   | 3.26G/4.95G [01:05<00:29, 56.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  66%|██████▋   | 3.28G/4.95G [01:05<00:32, 51.9MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  67%|██████▋   | 3.30G/4.95G [01:06<00:34, 48.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  67%|██████▋   | 3.31G/4.95G [01:06<00:33, 49.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  67%|██████▋   | 3.33G/4.95G [01:06<00:31, 52.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  68%|██████▊   | 3.34G/4.95G [01:06<00:28, 55.5MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  68%|██████▊   | 3.36G/4.95G [01:07<00:28, 56.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  68%|██████▊   | 3.38G/4.95G [01:07<00:28, 55.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  69%|██████▉   | 3.41G/4.95G [01:08<00:27, 55.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  70%|██████▉   | 3.44G/4.95G [01:08<00:27, 54.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  70%|██████▉   | 3.46G/4.95G [01:09<00:27, 53.7MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  70%|███████   | 3.47G/4.95G [01:09<00:27, 52.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors:  71%|███████   | 3.49G/4.95G [01:09<00:34, 42.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  71%|███████   | 3.50G/4.95G [01:10<00:32, 44.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  71%|███████   | 3.52G/4.95G [01:10<00:31, 45.3MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  71%|███████▏  | 3.54G/4.95G [01:10<00:29, 48.6MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  72%|███████▏  | 3.55G/4.95G [01:11<00:27, 50.1MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  73%|███████▎  | 3.62G/4.95G [01:12<00:23, 56.0MB/s]\n",
      "\n",
      "model-00002-of-00003.safetensors:  74%|███████▎  | 3.65G/4.95G [01:12<00:23, 56.3MB/s]\n",
      "\n",
      "model-00003-of-00003.safetensors: 100%|██████████| 3.59G/3.59G [01:13<00:00, 49.1MB/s]\n",
      "model-00001-of-00003.safetensors: 100%|██████████| 4.94G/4.94G [01:36<00:00, 50.9MB/s]\n",
      "model-00002-of-00003.safetensors: 100%|██████████| 4.95G/4.95G [01:37<00:00, 50.9MB/s]\n",
      "\n",
      "Upload 3 LFS files: 100%|██████████| 3/3 [01:37<00:00, 32.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/chloeliu/llama-2-7b-chat-horoscope/commit/ac1a6a76228867e21e15b0ea84dbb08f341fe984', commit_message='Upload tokenizer', commit_description='', oid='ac1a6a76228867e21e15b0ea84dbb08f341fe984', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
